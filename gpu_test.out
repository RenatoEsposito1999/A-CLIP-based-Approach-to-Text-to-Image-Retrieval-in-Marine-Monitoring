trainable params: 1,001,473 || all params: 152,278,785 || trainable%: 0.6577
Nome: base_model.model.logit_scale | Trainabile: True | Shape: torch.Size([])
Nome: base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.visual_projection.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.visual_projection.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_projection.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_projection.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.logit_scale | Trainabile: True | Shape: torch.Size([])
Nome: base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.vision_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight | Trainabile: True | Shape: torch.Size([768, 8])
Nome: base_model.model.visual_projection.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 768])
Nome: base_model.model.visual_projection.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Nome: base_model.model.text_projection.lora_A.default.weight | Trainabile: True | Shape: torch.Size([8, 512])
Nome: base_model.model.text_projection.lora_B.default.weight | Trainabile: True | Shape: torch.Size([512, 8])
Trainable parameters: 1,001,473
Total parameters: 152,278,785
Parametri ottimizzati:
TO DO TEST

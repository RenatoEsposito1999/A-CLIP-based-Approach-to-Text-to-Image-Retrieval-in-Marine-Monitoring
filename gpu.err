Seed set to 20241203
Using cache found in /root/.cache/torch/hub/facebookresearch_dinov2_main
/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)
  warnings.warn("xFormers is available (SwiGLU)")
/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)
  warnings.warn("xFormers is available (Attention)")
/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)
  warnings.warn("xFormers is available (Block)")
Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Traceback (most recent call last):
  File "/workspace/text-to-image-retrivial/train.py", line 157, in <module>
    train(batch_size=args.bs, lr=args.lr, dim=args.dim, dev=args.dev)
  File "/workspace/text-to-image-retrivial/train.py", line 144, in train
    trainer.fit(model, train_dataloader, val_dataloader)
  File "/root/miniconda3/envs/tesi/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 561, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/tesi/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 48, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/tesi/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 599, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/tesi/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1012, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/tesi/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1056, in _run_stage
    self.fit_loop.run()
  File "/root/miniconda3/envs/tesi/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 217, in run
    self.on_advance_end()
  File "/root/miniconda3/envs/tesi/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 470, in on_advance_end
    call._call_callback_hooks(trainer, "on_train_epoch_end", monitoring_callbacks=True)
  File "/root/miniconda3/envs/tesi/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 227, in _call_callback_hooks
    fn(trainer, trainer.lightning_module, *args, **kwargs)
  File "/root/miniconda3/envs/tesi/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py", line 329, in on_train_epoch_end
    self._save_topk_checkpoint(trainer, monitor_candidates)
  File "/root/miniconda3/envs/tesi/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py", line 387, in _save_topk_checkpoint
    raise MisconfigurationException(m)
lightning.fabric.utilities.exceptions.MisconfigurationException: `ModelCheckpoint(monitor='recall@5')` could not find the monitored key in the returned metrics: ['Train_loss', 'Train_batch_acc', '_LR', 'Val_loss', 'Val_batch_acc', 'all_recall@1', 'all_recall@5', 'all_recall@10', 'turtle_recall@1', 'turtle_recall@5', 'turtle_recall@10', 'epoch', 'step']. HINT: Did you call `log('recall@5', value)` in the `LightningModule`?
